# -*- coding: utf-8 -*-
"""Project_1_CodeClause

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d-UAXWpBczS2jJQ2GRn0ksjRh3MO7AeX

# Customer Segmentation Using K Clustering
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Dataset"""

df = pd.read_excel("/content/Online Retail.xlsx") #importing the dataframe

df.head() #exploring the 1st 5 rows of the df

print("Number of Columns in the Data: ", df.shape[1])
print("Number of Rows in the Data: ", df.shape[0])
df.shape #give the number of rows and columns

df.info() #gives infos about the type of values and the number of null values

df.isnull().sum() # Total number of null values for each column

df.duplicated().sum() #Total number of duplicated rows

"""## Exploratory Data Analysis (EDA)"""

des = df.describe().transpose() #usefull statistics about the df
des

country_count = df.Country.value_counts() #giving the countries and their count
country_count

sns.barplot(x=country_count.values, y=country_count.index)
plt.title("Countries frequency" )
plt.xlabel("Number of Repetitions")
plt.ylabel("country")
plt.show()

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%d.%f')
time_trend = df['InvoiceDate'].value_counts().head(10)

sns.barplot(x=time_trend.values, y=time_trend.index)
plt.title('Top 10 Most Frequent Time')
plt.xlabel('Number of Orders')
plt.ylabel('Time')
plt.show()

description_count = df["Description"].value_counts().head(10)

sns.barplot(x=description_count.values, y=description_count.index)
plt.title("Top 10 Frequent Items", fontsize=16)
plt.xlabel("Number of Orders")
plt.ylabel("Items")
plt.show()

numerical_columns = df.select_dtypes(include="number").columns # Select only numerical columns from the DataFrame
num_features = len(numerical_columns) # Determine the number of features (numerical columns)
# Set up a flexible subplot grid (rows and columns) (dynamically)
num_cols = 3
num_rows = int(np.ceil(num_features / num_cols))

plt.figure(figsize=(num_cols * 4, num_rows * 3))# Create the figure with the calculated grid size
# Loop through each numerical feature and create a histogram
for i, feature in enumerate(numerical_columns):
    plt.subplot(num_rows, num_cols, i + 1)  # subplot position
    sns.histplot(data=df, x=df[feature], bins=30, kde=True, color='blue') #histogram with KDE =True (Kernel Density Estimation) curve
    plt.title(f'{feature}', fontsize=10)
    plt.ylabel(feature, fontsize=9)

plt.tight_layout() # Adjust layout to prevent overlapping elements
plt.show()

df.drop_duplicates(inplace=True)#removing duplicated rows

df.dropna(inplace=True)# Removing missing values
df.isnull().sum()

# This code removes outliers from the "Quantity" and "UnitPrice" columns of a DataFrame (df)
# using the Interquartile Range (IQR) method. The IQR method identifies outliers by calculating
# the first quartile (Q1) and third quartile (Q3) and setting upper and lower limits as:
# Upper Limit = Q3 + 1.5 * IQR
# Lower Limit = Q1 - 1.5 * IQR
# Any data points outside these limits are considered outliers and are removed.
# The process is applied separately to "Quantity" and "UnitPrice".
q1 = df["Quantity"].quantile(0.30)
q3 = df["Quantity"].quantile(0.70)
iqr = q3 - q1

upper_limit = q3 + (1.5 * iqr)
lower_limit = q1 - (1.5 * iqr)

df = df.loc[(df["Quantity"] < upper_limit) & (df["Quantity"] > lower_limit)]
q1 = df["UnitPrice"].quantile(0.25)
q3 = df["UnitPrice"].quantile(0.65)
iqr = q3 - q1

upper_limit = q3 + (1.5 * iqr)
lower_limit = q1 - (1.5 * iqr)

df = df.loc[(df["UnitPrice"] < upper_limit) & (df["UnitPrice"] > lower_limit)]

"""## Encoding"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA

X = df[["Quantity", "UnitPrice", "Country"]].copy() #sleceting columns and creating a copy to avoid unintended modifications to the original df

# Initialize the LabelEncoder, which converts categorical values into numerical labels.
# Apply Label Encoding to the "Country" column, this replaces each unique country name with a corresponding integer.

encoder = LabelEncoder()
X["Country"] = encoder.fit_transform(X["Country"])

"""## Feature Engineering"""

X["total_price"] = X["Quantity"] * X["UnitPrice"]
X

corr = X.corr()

sns.heatmap(corr, annot=True, fmt=".2f", cmap="icefire")# fmt formats the displayed values to 2 decimal places
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()

scaler = StandardScaler()  # Initialize a StandardScaler
X = scaler.fit_transform(X)  # Apply standardization to X

"""## Machine Learning - Clustering"""

# List to store the Within-Cluster Sum of Squares (WCSS) for different cluster counts
wcss = []

for i in range(2, 11):
    # Initialize K-Means with 'i' clusters and a fixed random state for reproducibility
    kmeans = KMeans(n_clusters=i, random_state=20)

    # Fit K-Means to the data
    kmeans.fit(X)

    # Append the inertia (WCSS) to the list
    wcss.append(kmeans.inertia_)

# Visualizing the Elbow Method to determine the optimal number of clusters for K-Means.
# The Within-Cluster Sum of Squares (WCSS) is plotted against different cluster numbers (from 2 to 10).
# A sharp "elbow" in the plot indicates the optimal number of clusters, where adding more clusters does not significantly reduce WCSS.

plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Applying K-Means clustering to the dataset.
# This initializes a K-Means model with 5 clusters and a fixed random state for reproducibility.
# The model is trained on the feature matrix X, and each data point is assigned to a cluster.
# The assigned cluster labels are stored in a new column 'Cluster' in the DataFrame (df).
kmeans = KMeans(n_clusters=6, random_state=20)
df['Cluster'] = kmeans.fit_predict(X)

# Creating a pairplot to visualize the distribution and relationships between selected features.
sns.pairplot(df, hue='Cluster', vars=['Quantity', 'UnitPrice'], palette='icefire')
plt.show()

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
df['PCA1'], df['PCA2'] = X_pca[:, 0], X_pca[:, 1]

# Scatterplot with PCA components
plt.figure(figsize=(10, 6))
sns.scatterplot(x=df['PCA1'], y=df['PCA2'], hue=df['Cluster'], palette='icefire', s=60, edgecolor='k')
plt.title("PCA-Based Cluster Visualization")
plt.show()

"""# Use a sample of the data

**silhouette_score = how well each data point fits within its assigned cluster compared to other clusters**
"""

from sklearn.metrics import silhouette_score
import numpy as np

sample_indices = np.random.choice(len(X), size=50000, replace=False)
sample_X = X[sample_indices]
sample_labels = df["Cluster"].values[sample_indices]

score = silhouette_score(sample_X, sample_labels)
print(f"Silhouette Score (sampled): {score}")

